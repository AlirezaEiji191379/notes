{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#Introduction)\n",
    "    1. [History](#History)\n",
    "    2. [ML Applications](#Applications)\n",
    "2. [Types of Machine Learning](#Types)\n",
    "    1. [Supervised Learning](#Supervised)\n",
    "        1. [Classification](#Classification)\n",
    "        2. [Naïve Bayes Classifier Algorithm](#Naïve)\n",
    "    2. [Unsupervised Learning](#Unsupervised)\n",
    "3. [Bias and Variance](#Bias)\n",
    "    1. [What is Bias?](#What)\n",
    "    2. [What is variance?](#variance)\n",
    "    3. [Bias and Variance Trade-Off](#Trade)\n",
    "4. [OverFitting and Underfitting](#and)\n",
    "    1. [So what is overfitting?](#overfitting)\n",
    "    2. [What is underfiting?](#underfiting)\n",
    "    3. [and how to prevent overfitting?](#prevent)\n",
    "5. [Parameter Estiamtion](#Estiamtion)\n",
    "    1. [What is MLE?](#MLE)\n",
    "    2. [Waht is MAP?](#MAP)\n",
    "6. [Handle unseen events](#Handle)\n",
    "    1. [Laplace Smoothing](#Laplace)\n",
    "    2. [Linear interpolation](#Linear)\n",
    "7. [More about dataset](#More)\n",
    "    1. [Training Dataset](#Training)\n",
    "    2. [Validation Dataset](#Validation)\n",
    "    3. [Test Dataset](#Test)\n",
    "    4. [About the dataset split ratio](#ratio)\n",
    "8. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Introduction\"></a>\n",
    "<h1>Introduction</h1>\n",
    "<p>Humans are capable of learning by experiencing, but how about computers? Can they learn from past data and make correct decisions for future situations?</p>\n",
    "<img src= \"./images/intro_pic_1.png\"></img>\n",
    "<p>Machine learning technics allow computers to automatically learn from a historical dataset and improve its performance from experiences and enables them to predict things in a new situation without being explicitly programmed. Machine learning algorithms bring computer science and statistics together to build mathematical models from our past dataset that is known as training data and use these models for future decision. The diagram below shows the ML algorithm:</p>\n",
    "<img src= \"./images/intro_pic_2.png\"></img>\n",
    "<a name=\"History\"></a>\n",
    "<h2>History</h2>\n",
    "<p>The base idea behind machine learning has a long history, But the term of machine learning was first used by Arthur Samuel in 1959. Below we can see a timeline of ML in brief:</p>\n",
    "<li><b>1834:</b> Charles Babbage made the first computer that could be programmed by punched cards and all modern computers followed its logical structure.</li>\n",
    "<li><b>1936:</b> Alan Turing gave a theory about how machines can execute a set of instructions.</li>\n",
    "<li><b>1940:</b> First manually operated computer named \"ENIAC\" was invented.</li>\n",
    "<li><b>1943:</b> The first model of human neural network was modeled with electronic circuits.</li>\n",
    "<li><b>1950:</b> \"Computer Machinery and Intelligence,\" was published by Alan Turing on the topic of AI with this question, \"Can machines think?\".</li>\n",
    "<li><b>1952:</b> Arthur Samuel created a program to play checkers for an IBM computer that could play better by playing more.</li>\n",
    "<li><b>1959:</b> Arthur Samuel first used the term of “Machine Learning” and popularized it. In this year, the first neural network was used to solve a real-world problem to remove echoes over phone lines using an adaptive filter.</li>\n",
    "<li><b>1974 to 1980:</b> These years are known as AI winter that lots of people lost their interest from AI and governmental funding reduced.</li>\n",
    "<li><b>1985:</b> A neural network was developed named NETtalk that was able to correctly pronounce 20000 words in a week.</li>\n",
    "<li><b>1997:</b> The IBM’s computer named Deep Blue won a chess game against human chess expert Garry Kasparov.</li>\n",
    "<li><b>2006:</b> Geoffrey Hinton has first used name of \"deep learning\" instead of neural net research. </li>\n",
    "<li><b>2012:</b> Google developed a deep neural network system to distinguish cat and human images in a YouTube video.</li>\n",
    "<li><b>2016:</b> AlphaGo won a Go game against number one and number second player of Go.</li>\n",
    "<a name=\"Applications\"></a>\n",
    "<h2>ML Applications</h2>\n",
    "<p>The machine learning concepts are growing every day and we can see its usages everywhere. Some of the real-world applications of ML are:</p>\n",
    "<li><b>Image recognition:</b><p>One example can be Facebook friends tagging suggestion that when we upload a photo with our Facebook friends, then we can see some labels suggestions with their names. This project is called Deep Face which uses face detection and recognition algorithms.</p></li>\n",
    "<li><b>Speech Recognition:</b><p>This project converts your voice to text and is also known as \"Computer speech recognition.\" Some examples of this technology are Google assistant, Siri, Cortana, and Alexa.</p></li>\n",
    "<li><b>Traffic prediction:</b><p>Google Maps gathers data from all its users and sends them to its database and uses ML algorithms and technics to provide the shortest routes with the least traffic to its users.</p></li>\n",
    "<li><b>Product recommendations:</b><p>Amazon and Netflix use ML for their product recommendation systems. Every time a user opens an amazon page is able to see most relevant suggestion according to its last searches or other features that are used in dataset.</p></li>\n",
    "<a name=\"Types\"></a>\n",
    "<h1>Types of Machine Learning</h1>\n",
    "<p>There are three major recognized categories for ML:</p>\n",
    "<img src= \"./images/types_learning_1.png\"></img>\n",
    "<a name=\"Supervised\"></a>\n",
    "<h2>Supervised Learning</h2>\n",
    "<p>In this method computer is provided by a set of labeled (tagged) data which is called training data and tries to create a mathematical model according to it and learn to approximate the exact nature of the relationship between examples and their labels. By the time this process of training finished the supervised learning algorithm will be able to observe a new, never-before-seen example and predict a good label for it. Then we provide another set of data called sample data to test if the predictions are exact or not.  One example of supervised learning is Face Recognition that we have a system that takes a photo, finds faces, and guesses who that is in the photo (suggesting a tag) is a supervised process. It has multiple layers to it, finding faces and then identifying them, but is still supervised nonetheless.\n",
    "The supervised algorithms are grouped in two main categories:\n",
    "</p>\n",
    "<ul>\n",
    "    <li><b>Classification</b></li>\n",
    "    <li><b>Regression</b></li>\n",
    "</ul>\n",
    "<a name=\"Classification\"></a>\n",
    "<h3>Classification</h3>\n",
    "<p>In this algorithm machine learns from given training data and identifies the category of input data and classifies it into number of groups or classes. The main difference of this method and regression is that the output of classification algorithm is a label or category, but the output of a regression algorithm is a value. The classifiers can be binary that means we have only two possible outcomes, or multi-class that means we can have more than two labels or classes. In another hand we have two types of learners in a classification problem:</p>\n",
    "<li><b>Lazy Learners:</b><ul><p>These learners find the most related data that is stored in our training dataset. First they store the training data and start the next phase when the test dataset arrives. In this method we reduce the training time but prediction time increases.  Some algorithms of this method are:</p>\n",
    "<li>K-NN algorithm</li>\n",
    "<li>b.\tCase-based reasoning</li>\n",
    "</ul>    \n",
    "</li>\n",
    "<li><b>Eager Learners: </b><ul><p>In this method the learner creates a model according to the training dataset, so the training time is more than lazy learners but prediction is faster. Some algorithms of this method are:</p>\n",
    "<li>Decision Trees</li>\n",
    "<li>Naïve Bayes</li>\n",
    "<li>ANN</li>\n",
    "</ul>    \n",
    "</li>\n",
    "<a name=\"Naïve\"></a>\n",
    "<h3>Naïve Bayes Classifier Algorithm</h3>\n",
    "<p>Naïve Bayes algorithm is a probabilistic classifier based on Bayes theorem and is used to solve classification problems. It is called naïve because it assumes that occurrence of a certain feature is independent of the occurrence of other features. It is also called Bayes because it depends on the principle of Bayes' Theorem.</p>\n",
    "<ul>\n",
    "<h4>Advantages of Naïve Bayes Classifier</h4>\n",
    "<ul>\n",
    "    <li>It is fast and not that much complicated.</li>\n",
    "    <li>It can be used for Binary as well as Multi-class Classifications.</li>\n",
    "    <li>It is the most popular choice for text classification problems.</li>\n",
    "</ul>\n",
    "<h4>Disadvantages of Naïve Bayes Classifier</h4>\n",
    "<ul>\n",
    "    <li>Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features. It can be helpful in some cases to consider features independent, but sometimes the results are not exact.</li>\n",
    "</ul>    \n",
    "</ul>    \n",
    "<a name=\"Unsupervised\"></a>\n",
    "<h2>Unsupervised Learning</h2>\n",
    "<p>In this method our training dataset doesn’t contain labels and they have not been categorized. Instead, our algorithm would be fed a lot of data and given the tools to understand the properties of the data. In this case machine learns to group, cluster the data such that a human (or other intelligent algorithm) can come in and make sense of the newly organized data. In another word the goal of unsupervised learning is to find the underlying structure of dataset, group that data according to similarities, and represent that dataset in a compressed format.\n",
    "The supervised algorithms are grouped in two main categories:\n",
    "</p>\n",
    "<ul>\n",
    "    <li><b>Clustering</b></li>\n",
    "    <li><b>Association</b></li>\n",
    "</ul>    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"Bias\"></a>\n",
    "<h1>Bias and Variance</h1>\n",
    "\n",
    "<p>\n",
    "if the model in machine learning  is not accurate, it will make errors and these errors are known as Bias and Variance. these two errors shows small difference between predicted and actual value! In machine learning we want to reduce these errors to get more accurate predictions</p>\n",
    "<p> On the basis of these errors, the machine learning model is selected that can perform best on the particular dataset.</p>\n",
    "\n",
    "<a name=\"What\"></a>\n",
    "<h2>What is Bias?</h2>\n",
    "<p>\n",
    "In general, a machine learning model analyses the data, find patterns in it and make predictions. While training, the model learns these patterns in the dataset and applies them to test data for prediction.\n",
    "<b>While the model in making prediction about data, a difference occurs between prediction values made by the machine learning model and actual values is known as bias errors</b>\n",
    "It can be defined as an inability of ml algorithms such as polynomial Regression to capture the true relationship between the data points. Each algorithm in ml  has some amount of bias because bias occurs from assumptions in the model, which makes the target function simple to learn. A model has either:\n",
    "<ul>\n",
    "<li><b>High Bias:</b> A model with a high bias has more assumptions, and the model can not capture the important features of our dataset. the model with high biased has good performance on training data but not test data.</li>\n",
    "<li><b>Low Bias:</b> A low bias model has fewer assumptions about the form of the function of dataset.</li>\n",
    "</ul>\n",
    "linear algorithms have a high bias, as they learn fast. The simpler the algorithm, the higher the bias it has! but a nonlinear algorithm often has low bias.\n",
    "</p>\n",
    "\n",
    "<h4>Machine learning algorithm with low bias:</h4>\n",
    "<ul> \n",
    "<li>Decision Trees</li>\n",
    "<li>k-Nearest Neighbours</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Machine learning algorithm with high bias:</h4>\n",
    "<ul> \n",
    "<li>Linear Regression</li>\n",
    "<li>Logistic Regression</li>\n",
    "</ul>\n",
    "\n",
    "<a name=\"variance\"></a>\n",
    "<h2>What is variance?</h2>\n",
    "\n",
    "<p>The variance of model tells us the amount of variation in the prediction if the different data is used. a model should not vary too much from one training dataset to another, which means the algorithm should be good in understanding the mapping between inputs and outputs. some models has <b>low variance</b> but some of them has <b>high variance. </b></p>\n",
    "\n",
    "<p> <b>low varince</b> means that we have small variation in the changes of training data with prediction in target function!\n",
    "<b>high variance means there is a high variation in the prediction of the target function with changes in the training dataset</b> <p>\n",
    "\n",
    "<p>a model that has high variance is well-performed with training data but can not generalize unseen dataset as well.\n",
    "high variance leads to overfitting and increases model complexities!\n",
    "</p>\n",
    "\n",
    "<h4>machine learning algorithms with low variance:</h4>\n",
    "<ul>\n",
    "<li>Linear Regression</li>\n",
    "<li>Logistic Regression</li>\n",
    "</ul>\n",
    "\n",
    "<h4>machine learning algorithms with high variance:</h4>\n",
    "<ul>\n",
    "<li>decision tree</li>\n",
    "<li>K-nearest neighbours</li>\n",
    "</ul>\n",
    "\n",
    "in general variance shows us how much our model is dependent to our dataset!\n",
    "\n",
    "<p>in the following image you can see the impact of bias and variance in machine learning model:</p>\n",
    "<img src= \"./images/bias-and-variance-in-machine-learning4.png\"></img>\n",
    "\n",
    "<a name=\"Trade\"></a>\n",
    "<h3>Bias and Variance Trade-Off</h3>\n",
    "<p>the best and ideal model has low bias and low variance but it can not be acheived and is not possible practically!\n",
    "low-bias and high-variance leads to overfitting, high-bias and low-variance leads to unferfitting! so it is required to make a balance between bias and variance errors, and this balance between the bias error and variance error is known as the Bias-Variance trade-off.\n",
    "in simple words bias and variance are related to each other and we can not reduce both of them simultanously!</p>\n",
    "<img src=\"./images/bias-and-variance-in-machine-learning6.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"and\"></a>\n",
    "<h1>OverFitting and Underfitting</h1>\n",
    "<p>Supervised machine learning is best understood as approximating a target function (f) that maps input variables (X) to an output variable (Y). f(X) = Y</p>\n",
    "\n",
    "<p>An important consideration in learning the target function from the training data is how well the model generalizes to new data. Generalization is important because the data we collect is only a sample, it is incomplete and noisy</p>\n",
    "<ul>\n",
    "<li><b>Generalization:</b> It shows how well a model is trained to predict unseen data.</p></li>\n",
    "<li><b>Noise: </b> Noise is meaningless or irrelevant data present in the dataset. It affects the performance of the model if it is not removed.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The goal of a good machine learning model is to generalize well from the training data to any data from the problem domain. This allows us to make predictions in the future on data the model has never seen.</p>\n",
    "\n",
    "<p>There is a terminology used in machine learning when we talk about how well a machine learning model learns and generalizes to new data, namely overfitting and underfitting.</p>\n",
    "\n",
    "<p>Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms.</p>\n",
    "\n",
    "<a name=\"overfitting\"></a>\n",
    "<h2>So what is overfitting?</h2>\n",
    "<p>when the ml model predicts training data very well but it can not predict unseen data as well as training data we face overfitting in our ml algorithm! </br>\n",
    "This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.\n",
    "<b><i>An overfitted model is said to have low bias and high variance.</i></b>\n",
    "</p>\n",
    "<p>To detect overfitting we must split our dataset.about 80% of our data set is goinig to be trained and 20% of dataset is going to be tested!Now, if the model performs well with the training dataset but not with the test dataset, then it is likely to have an overfitting issue. for example if the error is 2.5% in training data and 20% in test data we have overfitting</p>\n",
    "\n",
    "<a name=\"underfiting\"></a>\n",
    "<h2>What is underfiting?</h2>\n",
    "<p>Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.</p>\n",
    "\n",
    "<a name=\"prevent\"></a>\n",
    "<h3>and how to prevent overfitting?</h3>\n",
    "<ul>\n",
    "<li><b>cross validation: </b>In the general k-fold cross-validation technique, we divided the dataset into k-equal-sized subsets of data; these subsets are known as folds.</li>\n",
    "<li><b>regularization: </b>in this technique we reduce our model features and simplify it with our tools! it may increase bias but reduce the variance of the model. in many cases we add penalizing term to our objective function. as an example in linear regression we use l2 regularization and add sum penalizing term to the MSE function</li>\n",
    "<li>...</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<img src=\"./images/overfitting-in-machine-learning.png\"></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Estiamtion\"></a>\n",
    "<h1>Parameter Estiamtion</h1>\n",
    "\n",
    "<a name=\"MLE\"></a>\n",
    "<h2>What is MLE?</h2>\n",
    "\n",
    "<p>The following is a general setup for a statistical inference problem: There is an unknown quantity that we would like to estimate. We get some data. From the data, we estimate the desired quantity this is frequentist approach to this problem. In this approach, the unknown quantity θ is assumed to be a fixed (non-random) quantity that is to be estimated by the observed data.</p>\n",
    "\n",
    "<p>We now would like to talk about a systematic way of parameter estimation. Specifically, we would like to introduce an estimation method, called maximum likelihood estimation (MLE)</p>\n",
    "<p>Let X1, X2, X3, ..., Xn be a random sample from a distribution with a parameter θ (In general, θ might be a vector, θ=(θ1,θ2,⋯,θk).) Suppose that x1, x2, x3, ..., xn are the observed values of X1, X2, X3, ..., Xn. If Xi's are discrete random variables, we define the likelihood function as the probability of the observed sample as a function of θ:<br>\n",
    "L(x1,x2,⋯,xn;θ) = P(X1=x1,X2=x2,⋯,Xn=xn;θ)\n",
    "\n",
    "<ol>\n",
    "<li>If Xi's are discrete, then the likelihood function is defined as\n",
    "<b>L(x1,x2,⋯,xn;θ) = PX1X2⋯Xn(x1,x2,⋯,xn;θ)</b>. </li>\n",
    "\n",
    "<li>If Xi's are jointly continuous, then the likelihood function is defined as\n",
    "<b>L(x1,x2,⋯,xn;θ)=fX1X2⋯Xn(x1,x2,⋯,xn;θ)</b>.</li>\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<p>In some problems, it is easier to work with the log likelihood function given by\n",
    "ln L(x1,x2,⋯,xn;θ).</p>\n",
    "\n",
    "<p>Now that we have defined the likelihood function, we are ready to define maximum likelihood estimation. Let X1, X2, X3, ..., Xn be a random sample from a distribution with a parameter θ. Suppose that we have observed X1=x1, X2=x2, ⋯, Xn=xn. The maximum likelihood estimate of θ, shown by θ^ML is the value that maximizes the likelihood function\n",
    "L(x1,x2,⋯,xn;θ).\n",
    "</p>\n",
    "\n",
    "<p>Note that the value of the maximum likelihood estimate is a function of the observed data. Thus, as any other estimator, the maximum likelihood estimator (MLE) is indeed a random variable</p>\n",
    "\n",
    "<a name=\"MAP\"></a>\n",
    "<h2>Waht is MAP?</h2>\n",
    "\n",
    "<p> we would like to discuss a different approach for inference, namely the Bayesian approach. In the Bayesian framework, we treat the unknown quantity, Θ, as a random variable. we assume that we have some initial guess about the distribution of Θ. This distribution is called the prior distribution. We observe some data. We then use Bayes' rule to make inference about the unobserved random variable. This is generally how we approach inference problems in Bayesian statistics.That is why this approach is called the Bayesian approach.</p>\n",
    "\n",
    "<p>in this approach our goal is to draw inferences about an unknown variable X by observing a related random variable Y. The unknown variable is modeled as a random variable X, with prior distribution.<br>\n",
    "After observing the value of the random variable Y, we find the posterior distribution of X. This is the conditional PDF (or PMF) of X given Y=y,\n",
    "we use bayes formula for calculating posterior distribution.\n",
    "</p>\n",
    "<img src='./images/formula.png'></img>\n",
    "\n",
    "<p>in the above image we have Fx|y(x|y) as postrior distribution and fx(X) as priror distribution. in many problems Fy|x(y|x) is MLE function that we explained it later!</p>\n",
    "\n",
    "<p>The posterior distribution, fX|Y(x|y), contains all the knowledge about the unknown quantity X. Therefore, we can use the posterior distribution to find point estimation of X. One way to obtain a point estimate is to choose the value of x that maximizes the posterior PDF (or PMF). This is called the maximum a posteriori (MAP) estimation</p>\n",
    "\n",
    "<p>Note that fY(y) does not depend on the value of x. Therefore, we can equivalently find the value of x that maximizes:\n",
    "    <br>fY|X(y|x)fX(x)</br>\n",
    "</p>\n",
    "\n",
    "<p><br>in many cases we choose alpha-beta distribution for prior distribution</br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Handle\"></a>\n",
    "<h1>Handle unseen events</h1>\n",
    "\n",
    "<p>One of the most important concepts in machine learning is to handle unseen events. To clarify, consider the following example.\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>Consider the vector $X=[x_1,x_2,x_3,x_4,x_5]$ and the following data as training dataset.</p>\n",
    "\n",
    "</br>\n",
    "\n",
    "|    data     |    label    |\n",
    "| :-----------: | :-----------: |\n",
    "|[1, 1, 1, 0, 1]|    a    |\n",
    "|[1, 0, 1, 0, 0]|    a    |\n",
    "|[0, 0, 1, 0, 1]|    a    |\n",
    "|[0, 1, 1, 0, 0]|    a    |\n",
    "|[0, 0, 1, 0, 0]|    a    |\n",
    "|[0, 0, 0, 0, 1]|    b    |\n",
    "|[0, 1, 1, 0, 1]|    b    |\n",
    "|[0, 1, 0, 1, 1]|    b    |\n",
    "|[0, 1, 1, 1, 1]|    b    |\n",
    "|[0, 0, 0, 1, 1]|    b    |\n",
    "\n",
    "<p>We want to predict the label of $[1, 1, 0, 0, 1]$ using a simple naïve Bayes model. First, we create a naïve Bayes model based on the training dataset.<p>\n",
    "    \n",
    "<img src='./images/naive_model_1.png'></img>\n",
    "\n",
    "<p>Now, we calculate the probability of each label based on the created naïve Bayes model.</p>\n",
    "\n",
    "</br>\n",
    "\n",
    "$P\\left(label=a\\right)\\propto\\frac{2}{5}\\times\\frac{2}{5}\\times0\\times1\\times\\frac{2}{5}=0$\n",
    "\n",
    "</br>\n",
    "\n",
    "$P\\left(label=b\\right)\\propto0\\times\\frac{3}{5}\\times\\frac{3}{5}\\times\\frac{2}{5}\\times1=0$\n",
    "\n",
    "<p>If you look at the naïve bayes model again, you will find out that there is no data with $x_3=0$ or $x_4=1$ for label a, and there is no data with $x_1=1$ or $x_5=0$ for label b.</p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<b>So, what would we do to handle this situation?</b>\n",
    "\n",
    "<a name=\"Laplace\"></a>\n",
    "<h2>Laplace Smoothing</h2>\n",
    "<p>One of the most common methods to solve the above problem is using Laplace smoothing. Consider a bag of green and yellow balls. Suppose there are three green balls and no yellow balls in the bag. So, the probability of each color is:\n",
    "</p>\n",
    "<img src='./images/balls_1.png', width=200></img>\n",
    "$P\\left(color=green\\right)=1$\n",
    "\n",
    "</br>\n",
    "\n",
    "$P\\left(color=yellow\\right)=0$\n",
    "<p>Using Laplace smoothing, we pretend that we saw every outcome extra time. For the above example, by using Laplace smoothing with, the probabilities for each color are as follows:</p>\n",
    "<img src='./images/balls_2.png', width=350></img>\n",
    "$P\\left(color=green\\right)=\\frac{4}{5}$\n",
    "\n",
    "</br>\n",
    "\n",
    "$P\\left(color=yellow\\right)=\\frac{1}{5}$\n",
    "\n",
    "<p>In general:</p>\n",
    "$P_{LAP,k}\\left(x\\right)=\\frac{c\\left(x\\right)+k}{N+k\\left|X\\right|}$\n",
    "\n",
    "</br>\n",
    "\n",
    "<p>And for conditional probabilities:</p>\n",
    "$P_{LAP,k}\\left(x|y\\right)=\\frac{c\\left(x,y\\right)+k}{c\\left(y\\right)+k\\left|X\\right|}$\n",
    "\n",
    "<b>Using Laplace smoothing with large k is not appropriate as it ignores the prior belief.</b>\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p>Now, back to the label prediction example. We use Laplace smoothing with $k=1$ and calculate label probabilities again. We consider the following extra data:</p>\n",
    "\n",
    "|    data     |    label    |\n",
    "| :-----------: | :-----------: |\n",
    "|[1, 1, 1, 1, 1]|    a    |\n",
    "|[1, 1, 1, 1, 1]|    b    |\n",
    "|[0, 0, 0, 0, 0]|    a    |\n",
    "|[0, 0, 0, 0, 0]|    b    |\n",
    "\n",
    "<img src='./images/naive_model_2.png'></img>\n",
    "\n",
    "$P\\left(label=a\\right)\\propto\\frac{3}{7}\\times\\frac{3}{7}\\times\\frac{1}{7}\\times\\frac{6}{7}\\times\\frac{3}{7}=\\frac{162}{16807}$\n",
    "\n",
    "</br>\n",
    "\n",
    "$P\\left(label=b\\right)\\propto\\frac{1}{7}\\times\\frac{4}{7}\\times\\frac{4}{7}\\times\\frac{3}{7}\\times\\frac{6}{7}=\\frac{288}{16807}$\n",
    "\n",
    "\n",
    "<a name=\"Linear\"></a>\n",
    "<h2>Linear interpolation</h2>\n",
    "\n",
    "<p>In practice, Laplace smoothing often performs poorly for $P(X|Y)$ When $X$ or $Y$ is very large.</p>\n",
    "\n",
    "<p>Another useful method is <b>linear interpolation</b>. Linear interpolation (also known as <b>Lerp</b>) is a method to find unknown values between two known points. The unknown values are approximated through Linear interpolation by connecting these two known points with a straight line.</p>\n",
    "\n",
    "<p>In this method, we consider both $p(x|y)$ and $p(x)$ to calculate the $p(x|y)$</p>\n",
    "</br>\n",
    "<p>In general:</p>\n",
    "$P_{LIN}\\left(x\\middle|\\ y\\right)=\\alpha P\\left(x\\middle|\\ y\\right)+\\left(1-\\alpha\\right)P\\left(x\\right),\\ 0<\\alpha<1$\n",
    "\n",
    "<p>By choosing an appropriate value for $\\alpha$, we can handle unseen events.</p>\n",
    "\n",
    "\n",
    "<a name=\"More\"></a>\n",
    "<h1>More about dataset</h1>    \n",
    "\n",
    "<p>In the last section, we learned about Laplace smoothing and linear interpolation. But how can we calculate appropriate values for $\\alpha$ and k?</p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<p>We introduce three datasets:</p>\n",
    "\n",
    "<a name=\"Training\"></a>\n",
    "<h2>Training Dataset</h2>\n",
    "<b>The sample of data used to fit the model.</b>\n",
    "<p>The actual dataset that we use to train the model (weights and biases in the case of a Neural Network). The model sees and learns from this data.</p>\n",
    "\n",
    "<a name=\"Validation\"></a>\n",
    "<h2>Validation Dataset</h2>\n",
    "<b>The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.</b>\n",
    "<p>The validation set is used to evaluate a given model, but this is for frequent evaluation. We, as machine learning engineers, use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data but never does it “Learn” from this. We use the validation set results and update higher-level hyperparameters. So, the validation set affects a model, but only indirectly. The validation set is also known as the Dev set or the Development set. This makes sense since this dataset helps during the “development” stage of the model. We use this dataset to calculate $\\alpha$ and $k$.</p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src='./images/diagram.png' style=\"width:400px;\"/>\n",
    "\n",
    "</br>\n",
    "\n",
    "<a name=\"Test\"></a>\n",
    "<h2>Test Dataset</h2>\n",
    "<b>The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.</b>\n",
    "<p>The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained(using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set, and the actual test set is only released when the competition is about to close, and it is the result of the model on the Test set that decides the winner). Many times the validation set is used as the test set, but it is not good practice. The test set is generally well-curated. It contains carefully sampled data that spans the various classes that the model would face when used in the real world.</p>\n",
    "\n",
    "<a name=\"ratio\"></a>\n",
    "<h2>About the dataset split ratio</h2>\n",
    "<p>Now that you know what these datasets do, you might be looking for recommendations on how to split your dataset into Train, Validation and Test sets.</p></br>\n",
    "<p>This mainly depends on 2 things. First, the total number of samples in your data and second, on the actual model you are training.</p></br>\n",
    "<p>Some models need substantial data to train upon, so in this case, you would optimize for the larger training sets. Models with very few hyperparameters will be easy to validate and tune, so you can probably reduce the size of your validation set, but if your model has many hyperparameters, you would want to have a large validation set as well(although you should also consider cross-validation). Also, if you happen to have a model with no hyperparameters or ones that cannot be easily tuned, you probably don’t need a validation set too!</p>\n",
    "\n",
    "<img src='./images/ratio.png' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Conclusion\"></a>\n",
    "<h1>Conclusion</h1>\n",
    "<p>so in this chapter, we introduced key concepts of machine learning.\n",
    "we learned about the applications of ML, supervised and unsupervised in ML, and some algorithms like naive Bayes.\n",
    "after these primary concepts, we learned about some problems like bias and variance and their relations to overfitting and underfitting.\n",
    "for parameter estimation, we learned two statistical approaches like Bayesian and frequentist approach and after that, we used these two frameworks and some formulas like Laplace and linear interpolation which are based on MLE and MAP.\n",
    "we also learned about how to work with datasets in machine learning</p>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
